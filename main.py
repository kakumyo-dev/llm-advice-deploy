from flask import Flask, jsonify
from google.cloud import bigquery
from dotenv import load_dotenv
import os
import openai
print(f"âœ… openai version: {openai.__version__}")
from openai import OpenAI
import json
from datetime import date

app = Flask(__name__)

print("âœ… Flask app initialized")

load_dotenv()  # .env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ç’°å¢ƒå¤‰æ•°ã«åæ˜ 
print("âœ… .env loaded")

api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    print("âŒ OPENAI_API_KEY is not set")
else:
    print(f"âœ… OPENAI_API_KEY loaded: {api_key[:5]}***")

@app.route("/")
def index():
    try:
        print("ðŸ”„ Initializing OpenAI client...")
        openai_client = OpenAI(api_key=api_key)
        print("âœ… OpenAI client initialized")

        bigquery_client = bigquery.Client()
        query = """
WITH sleep AS (
  SELECT 
    summary_date,
    participant_uid,
    score,
    total,
    light,
    rem,
    deep
  FROM `sic-ouraring-verify.gcube.sleep`
  WHERE total >= 3600
),
activity AS (
  SELECT 
    summary_date,
    participant_uid,
    non_wear,
    inactive,
    inactivity_alerts,
    steps
  FROM `sic-ouraring-verify.gcube.activity`
  WHERE non_wear <= 14400
)

SELECT 
  s.summary_date AS date,
  s.participant_uid AS id,
  s.score AS sleep_score,
  s.total AS total_sleep_seconds,
  s.light AS light_sleep_seconds,
  s.rem AS rem_sleep_seconds,
  s.deep AS deep_sleep_seconds,
  a.steps,
FROM sleep s
LEFT JOIN activity a
  ON s.summary_date = a.summary_date AND s.participant_uid = a.participant_uid
WHERE s.summary_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) AND LAST_DAY(CURRENT_DATE())
ORDER BY s.participant_uid ASC,s.summary_date ASC
LIMIT 1000
        """
        query_job = bigquery_client.query(query)
        results = query_job.result()
        data_list = [dict(row.items()) for row in results]

        prompt_data = "\n".join([str(row) for row in data_list])
        print(f"ðŸ“‹ Prompt data prepared: {prompt_data}")

        # OpenAI GPT-4o ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": """ã‚ãªãŸã¯å¥åº·çŠ¶æ…‹ã‚’ç®¡ç†ã™ã‚‹ãƒ™ãƒ†ãƒ©ãƒ³ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãã®äººã®å¥åº·çŠ¶æ…‹ã‚’çŸ¥ã‚Šã€é©åˆ‡ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã™ã‚‹ã“ã¨ãŒã‚ãªãŸã®ä»•äº‹ã§ã™ã€‚
ç”Ÿä½“æƒ…å ±(æ™‚é–“ã€é‹å‹•é‡)ã‚’æŒã£ã¦ã„ã¾ã™ã€‚
sleep_scoreã¯ã€æ•°å€¤ãŒé«˜ã„ã»ã©è³ªã®é«˜ã„ç¡çœ ãŒã§ãã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
total_sleep_secondsã¯æ·±ã„ç¡çœ ã€light_sleep_secondsã¯æµ…ã„ç¡çœ ã€rem_sleep_secondsã¯ãƒ¬ãƒ ç¡çœ ã‚’è¡¨ã—ã¾ã™ã€‚
stepsã¯1æ—¥ã®æ­©æ•°ã‚’è¡¨ã—ã¾ã™ã€‚

ã“ã‚Œã‹ã‚‰æ‰±ã†ç”Ÿä½“æƒ…å ±ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿æ›´æ–°ãŒé€”ä¸­ã§æ­¢ã¾ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€
æ¬ æå€¤ã‚„é€£ç¶šã—ãŸåŒã˜å€¤ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€
ä½¿ãˆã‚‹ç¯„å›²ã§ãƒ‡ãƒ¼ã‚¿åˆ†æžã‚’è¡Œã„ã€å¿…è¦ä»¥ä¸Šã«æ°—ã«ã—ã™ãŽãªã„ã§ãã ã•ã„ã€‚
                 
ä»¥ä¸‹ã®å½¢å¼ã®JSONã§idã”ã¨ã«åˆ†ã‘ã¦å›žç­”ã—ã¦ãã ã•ã„ï¼š
{
    "id": "id",
    "sleep_analysis": "ç¡çœ ã«é–¢ã™ã‚‹åˆ†æžã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹",
    "activity_analysis": "æ­©æ•°ã«é–¢ã™ã‚‹åˆ†æžã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹",
    "recommendations": "å…·ä½“çš„ãªæ”¹å–„ææ¡ˆ",
    "overall_assessment": "ç·åˆçš„ãªè©•ä¾¡"
}
                 
æœ€çµ‚å¿œç­”ã¯ã€"{"ã§å§‹ã¾ã‚Š"}"ã§çµ‚ã‚ã‚‹ã€‚ã¾ãŸã¯"["ã§å§‹ã¾ã‚Š"]"ã§çµ‚ã‚ã‚‹JSONã®ã¿ã‚’å‡ºåŠ›ã—ã€JSONä»¥å¤–ã®æ–‡å­—ã¯ä¸€åˆ‡å¿œç­”ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚"""},
                {"role": "user", "content": f"""ä»Šã‚ã‚‹äººã®1é€±é–“åˆ†ã®ç”Ÿä½“æƒ…å ±(æ™‚é–“ã€é‹å‹•é‡)ã¨
1ãƒ¶æœˆã®ç”Ÿä½“æƒ…å ±ã‚’æŒã£ã¦ã„ã¾ã™ã€‚

ç›´è¿‘ã®1é€±é–“åˆ†ã¨1ãƒ¶æœˆåˆ†ã®ç”Ÿä½“æƒ…å ±ã®é•ã„ãŒã‚ã‚Œã°ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜Žã—ã¦ãã ã•ã„ã€‚
ç›¸æ‰‹ã¯å°‚é–€å®¶ã§ã¯ãªãä¸€èˆ¬ã®äººãªã®ã§ã€æ•°å€¤ã ã‘ã«é ¼ã‚‰ãšã€
é‹å‹•é‡ã€ç¡çœ æ™‚é–“ãªã©ã‚’ä¸å¯§ã«æ¯”è¼ƒã—ã€
ã‚ˆã‚Šåˆ†ã‹ã‚Šã‚„ã™ã„æ–‡ç« ã§èª¬æ˜Žã‚’è¡Œã£ã¦ãã ã•ã„ã€‚ï¼š\n\n{prompt_data}"""}
            ],
                timeout=300  # æœ€å¤§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤ï¼ˆ5åˆ†ï¼‰ã‚’è¨­å®š
        )

        llm_advice_content = response.choices[0].message.content
        print(f"ðŸ’¬ GPT response: {llm_advice_content}")

        # ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯è¨˜æ³•ã‚’é™¤åŽ»
        llm_advice_content = llm_advice_content.replace("```json", "").replace("```", "").strip()

        # JSONæ–‡å­—åˆ—ã‚’Pythonã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        try:
            advice_data = json.loads(llm_advice_content)
            print(f"âœ… JSON parsed successfully: {type(advice_data)}")
        except json.JSONDecodeError as e:
            print(f"âŒ Failed to parse JSON response: {e}")
            print(f"âŒ Raw response: {llm_advice_content}")
            return jsonify({"error": "Invalid JSON response from GPT"}), 500

        # BigQueryã«ä¿å­˜
        table_id = "llm_advicebot.llm_advice_makino"
        
        # advice_dataãŒãƒªã‚¹ãƒˆã®å ´åˆã¯å„è¦ç´ ã‚’å‡¦ç†ã€è¾žæ›¸ã®å ´åˆã¯ãƒªã‚¹ãƒˆã«å¤‰æ›
        if isinstance(advice_data, list):
            advice_list = advice_data
        elif isinstance(advice_data, dict):
            advice_list = [advice_data]
        else:
            print(f"âŒ Unexpected data structure: {type(advice_data)}")
            return jsonify({"error": "Unexpected response structure from GPT"}), 500

        # å„IDã®ãƒ‡ãƒ¼ã‚¿ã‚’BigQueryã«ä¿å­˜
        rows_to_insert = []
        for advice_item in advice_list:
            if isinstance(advice_item, dict) and "id" in advice_item:
                row = {
                    "summary_date": date.today().isoformat(),
                    "participant_uid": advice_item.get("id", ""),
                    "sleep_analysis": advice_item.get("sleep_analysis", ""),
                    "activity_analysis": advice_item.get("activity_analysis", ""),
                    "recommendations": advice_item.get("recommendations", ""),
                    "overall_assessment": advice_item.get("overall_assessment", "")
                }
                rows_to_insert.append(row)
                print(f"âœ… Prepared data for ID: {advice_item.get('id', '')}")
            else:
                print(f"âŒ Skipping invalid advice item: {advice_item}")

        if rows_to_insert:
            print(f"ðŸ”„ Inserting {len(rows_to_insert)} records to BigQuery")
            errors = bigquery_client.insert_rows_json(table_id, rows_to_insert)
            if errors:
                print(f"âŒ Failed to insert rows: {errors}")
                return jsonify({"error": "BigQuery insert failed", "details": errors}), 500
            print(f"âœ… {len(rows_to_insert)} records saved to BigQuery")
        else:
            print("âŒ No valid data to insert")

        # GPTã®å¿œç­”ã‚’è¿”å´
        return jsonify(advice_data)
    except Exception as e:
        print(f"âŒ Exception occurred: {e}")
        return jsonify({"error": str(e)}), 500