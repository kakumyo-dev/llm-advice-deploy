from flask import Flask, jsonify, request
from google.cloud import bigquery
from dotenv import load_dotenv
import os
import openai
print(f"âœ… openai version: {openai.__version__}")
from openai import OpenAI
import json
from datetime import date
import httpx

app = Flask(__name__)

print("âœ… Flask app initialized")

load_dotenv()  # .env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ç’°å¢ƒå¤‰æ•°ã«åæ˜ 
print("âœ… .env loaded")

api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    print("âŒ OPENAI_API_KEY is not set")
else:
    print(f"âœ… OPENAI_API_KEY loaded: {api_key[:5]}***")

@app.route("/")
def index():
    participant_id = request.args.get("id")  # ä¾‹: /?id=user123
    try:
        print("ðŸ”„ Initializing OpenAI client...")
        openai_client = OpenAI(
            api_key=api_key, 
            timeout=httpx.Timeout(300.0, read=60.0, write=200.0, connect=20.0)
        )
        print("âœ… OpenAI client initialized")

        bigquery_client = bigquery.Client()
        query = f"""
WITH sleep AS (
  SELECT 
    summary_date,
    participant_uid,
    score,
    total,
    light,
    rem,
    deep,
    hr_average,
    hr_lowest,
    rmssd
  FROM `sic-ouraring-verify.gcube.sleep`
  WHERE total >= 3600
),
activity AS (
  SELECT 
    summary_date,
    participant_uid,
    steps,
    high,
    medium,
    low,
    inactive,
  	cal_total
  FROM `sic-ouraring-verify.gcube.activity`
  WHERE non_wear <= 14400
)

SELECT 
  s.summary_date AS date,
  s.score AS sleep_score,
  s.total AS total_sleep_seconds,
  s.light AS light_sleep_seconds,
  s.rem AS rem_sleep_seconds,
  s.deep AS deep_sleep_seconds,
  s.hr_average AS sleep_heart_rate_average,
  s.hr_lowest AS sleep_heart_rate_lowest,
  s.rmssd AS sleep_rmssd,
  a.steps,
  a.high AS high_intensity_activity_minutes,
  a.medium AS medium_intensity_activity_minutes,
  a.low AS low_intensity_activity_minutes,
  a.inactive AS inactivity_minutes,
  a.cal_total AS calorie_total
FROM sleep s
LEFT JOIN activity a
  ON s.summary_date = a.summary_date AND s.participant_uid = a.participant_uid
WHERE s.summary_date BETWEEN "2025-07-01" AND "2025-07-31" {"AND s.participant_uid = @participant_id" if participant_id else ""}
ORDER BY s.participant_uid ASC,s.summary_date ASC
LIMIT 100
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("participant_id", "STRING", participant_id)
            ] if participant_id else []
        )
        query_job = bigquery_client.query(query, job_config=job_config)
        results = query_job.result()
        data_list = [dict(row.items()) for row in results]

        prompt_data = "\n".join([str(row) for row in data_list])
        print(f"ðŸ“‹ Prompt data prepared: {prompt_data}")

        try:
            print(f"ðŸ”„ Sending request to OpenAI...")
            # OpenAI GPT-4o ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": f"""ã‚ãªãŸã¯å¥åº·çŠ¶æ…‹ã‚’ç®¡ç†ã™ã‚‹ãƒ™ãƒ†ãƒ©ãƒ³ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãã®äººã®å¥åº·çŠ¶æ…‹ã‚’çŸ¥ã‚Šã€é©åˆ‡ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã™ã‚‹ã“ã¨ãŒã‚ãªãŸã®ä»•äº‹ã§ã™ã€‚
ã‚ãªãŸã¯ç”Ÿä½“æƒ…å ±ã‚’æŒã£ã¦ã„ã¾ã™ã€‚
sleep_scoreã¯ã€æ•°å€¤ãŒé«˜ã„ã»ã©è³ªã®é«˜ã„ç¡çœ ãŒã§ãã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
deep_sleep_secondsã¯N3ï¼ˆæœ€ã‚‚æ·±ã„ãƒŽãƒ³ãƒ¬ãƒ ç¡çœ ï¼‰çŠ¶æ…‹ã®æ·±ã„ç¡çœ æ™‚é–“(ç§’)ã‚’è¡¨ã—ã¾ã™ã€‚light_sleep_secondsã¯N2ï¼ˆä¸­é–“ã®æ·±ã•ã®ãƒŽãƒ³ãƒ¬ãƒ ç¡çœ ï¼‰ã‚‚ã—ãã¯N1ï¼ˆæµ…ã„ãƒŽãƒ³ãƒ¬ãƒ ç¡çœ ï¼‰çŠ¶æ…‹ã®æµ…ã„ç¡çœ æ™‚é–“(ç§’)ã€rem_sleep_secondsã¯REMï¼ˆãƒ¬ãƒ ç¡çœ ï¼‰çŠ¶æ…‹ã®ç¡çœ æ™‚é–“(ç§’)ã‚’è¡¨ã—ã¾ã™ã€‚total_sleep_secondsã¯åˆè¨ˆç¡çœ æ™‚é–“(ç§’)ã‚’è¡¨ã—ã¾ã™ã€‚
sleep_heart_rate_averageã¯ç¡çœ ä¸­ã®å¿ƒæ‹æ•°ã®å¹³å‡å€¤ã‚’è¡¨ã—ã¾ã™ã€‚
sleep_heart_rate_lowestã¯ç¡çœ ä¸­ã®å¿ƒæ‹æ•°ã®æœ€ä½Žå€¤ã‚’è¡¨ã—ã¾ã™ã€‚
sleep_rmssdã¯ç¡çœ ä¸­ã®å¿ƒæ‹æ•°ã®å¤‰å‹•ã®å¹³å‡ã‚’è¡¨ã—ã¾ã™ã€‚
stepsã¯1æ—¥ã®æ­©æ•°ã‚’è¡¨ã—ã¾ã™ã€‚
high_intensity_activity_minutesã¯è‡ªè»¢è»Šã«ä¹—ã‚‹ç¨‹åº¦ã®é«˜å¼·åº¦é‹å‹•æ™‚é–“(åˆ†)ã€medium_intensity_activity_minutesã¯ã‚¦ã‚©ãƒ¼ã‚­ãƒ³ã‚°ç¨‹åº¦ã®ä¸­å¼·åº¦é‹å‹•æ™‚é–“(åˆ†)ã€low_intensity_activity_minutesã¯ç«‹ã£ã¦ã„ã‚‹ç¨‹åº¦ã®ä½Žå¼·åº¦é‹å‹•æ™‚é–“(åˆ†)ã€inactive_minutesã¯åº§ã£ã¦ã„ã‚‹ç¨‹åº¦ã®éžæ´»å‹•æ™‚é–“(åˆ†)ã‚’è¡¨ã—ã¾ã™ã€‚
calorie_totalã¯1æ—¥ã®ã‚«ãƒ­ãƒªãƒ¼æ¶ˆè²»é‡ã‚’è¡¨ã—ã¾ã™ã€‚

ã“ã‚Œã‹ã‚‰æ‰±ã†ç”Ÿä½“æƒ…å ±ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿æ›´æ–°ãŒé€”ä¸­ã§æ­¢ã¾ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€
æ¬ æå€¤ã‚„é€£ç¶šã—ãŸåŒã˜å€¤ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€
ä½¿ãˆã‚‹ç¯„å›²ã§ãƒ‡ãƒ¼ã‚¿åˆ†æžã‚’è¡Œã„ã€å¿…è¦ä»¥ä¸Šã«æ°—ã«ã—ã™ãŽãªã„ã§ãã ã•ã„ã€‚
                 
ä»¥ä¸‹ã®å½¢å¼ã®JSONã§å›žç­”ã—ã¦ãã ã•ã„ï¼š
{{
    "id": "{participant_id or 'unknown'}",
    "sleep_analysis": "ç¡çœ ã«é–¢ã™ã‚‹åˆ†æžã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹",
    "activity_analysis": "é‹å‹•ã«é–¢ã™ã‚‹åˆ†æžã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹",
    "recommendations": "å…·ä½“çš„ãªæ”¹å–„ææ¡ˆ",
    "overall_assessment": "ç·åˆçš„ãªè©•ä¾¡"
}}
                 
idã¯æŒ‡å®šã—ãŸæ–‡å­—åˆ—ã‚’ãã®ã¾ã¾ã«ã—ã¦å¤‰æ›´ã—ãªã„ã§ãã ã•ã„ã€‚
æœ€çµ‚å¿œç­”ã¯ã€"{{"ã§å§‹ã¾ã‚Š"}}"ã§çµ‚ã‚ã‚‹ã€‚ã¾ãŸã¯"["ã§å§‹ã¾ã‚Š"]"ã§çµ‚ã‚ã‚‹JSONã®ã¿ã‚’å‡ºåŠ›ã—ã€JSONä»¥å¤–ã®æ–‡å­—ã¯ä¸€åˆ‡å¿œç­”ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚"""},
                    {"role": "user", "content": f"""ã‚ã‚‹ä¼æ¥­ã®å¾“æ¥­å“¡ã®1ãƒ¶æœˆåˆ†ã®ç”Ÿä½“æƒ…å ±ã‚’æŒã£ã¦ã„ã¾ã™ã€‚
ç”Ÿä½“æƒ…å ±ã‚’ä½¿ã£ã¦ã€åˆ†æžçµæžœã®å›žç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ç¡çœ ã«é–¢ã™ã‚‹åˆ†æžã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã«ã¯ã€ç¡çœ æ™‚ã®ç”Ÿä½“æƒ…å ±ã®åˆ†æžçµæžœã¨ã€çµæžœã‚’ã‚‚ã¨ã«ã—ãŸãƒã‚¸ãƒ†ã‚£ãƒ–ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æ ¹æ‹ ã‚‚è¸ã¾ãˆã¦èª¬æ˜Žã—ã¦ãã ã•ã„ã€‚
é‹å‹•ã«é–¢ã™ã‚‹åˆ†æžã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã«ã¯ã€é‹å‹•æ™‚ã®ç”Ÿä½“æƒ…å ±ã®åˆ†æžçµæžœã¨ã€çµæžœã‚’ã‚‚ã¨ã«ã—ãŸãƒã‚¸ãƒ†ã‚£ãƒ–ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æ ¹æ‹ ã‚‚è¸ã¾ãˆã¦èª¬æ˜Žã—ã¦ãã ã•ã„ã€‚
å…·ä½“çš„ãªæ”¹å–„ææ¡ˆã«ã¯ã€ä¾‹ãˆã°ã€ç¡çœ æ™‚é–“ãŒã“ã‚Œã¾ã§ã‚ˆã‚ŠçŸ­ã„ãªã‚‰ã€ã‚‚ã†å°‘ã—é•·ãå¯ã‚‹å·¥å¤«ã‚’ä¿ƒã™ãªã©ã€å…·ä½“çš„ã«ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ–¹å‘æ€§ã‚’ä¼ãˆã¦ãã ã•ã„ã€‚
ç·åˆçš„ãªè©•ä¾¡ã«ã¯ã€æ ¹æ‹ ã‚„ãƒã‚¸ãƒ†ã‚£ãƒ–ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç››ã‚Šè¾¼ã¿ã€éŽã”ã—æ–¹ã«ã¤ã„ã¦ç·åˆçš„ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
ç›¸æ‰‹ã¯å°‚é–€å®¶ã§ã¯ãªãä¸€èˆ¬ã®äººãªã®ã§ã€æ•°å€¤ã ã‘ã«é ¼ã‚‰ãšã€ã‚ˆã‚Šåˆ†ã‹ã‚Šã‚„ã™ã„æ–‡ç« ã§èª¬æ˜Žã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
                     ãã‚Œãžã‚Œ400ï½ž600æ–‡å­—ç¨‹åº¦ã®è‡ªç„¶ãªæ—¥æœ¬èªžã§ææ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
                     \nç”Ÿä½“æƒ…å ±ï¼š\n\n{prompt_data}"""}
                ]
            )
            print(f"âœ… OpenAI response received")

        except openai.APITimeoutError as e:
            print(f"âŒ OpenAI API timeout error: {e}", flush=True)
            return jsonify({"error": "OpenAI API timeout"}), 408
        except Exception as e:
            print(f"âŒ OpenAI API error: {e}")
            return jsonify({"error": f"OpenAI API error: {str(e)}"}), 500

        llm_advice_content = response.choices[0].message.content
        print(f"ðŸ’¬ GPT response: {llm_advice_content}")

        # ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯è¨˜æ³•ã‚’é™¤åŽ»
        llm_advice_content = llm_advice_content.replace("```json", "").replace("```", "").strip()

        # JSONæ–‡å­—åˆ—ã‚’Pythonã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        try:
            advice_data = json.loads(llm_advice_content)
            print(f"âœ… JSON parsed successfully: {type(advice_data)}")
        except json.JSONDecodeError as e:
            print(f"âŒ Failed to parse JSON response: {e}")
            print(f"âŒ Raw response: {llm_advice_content}")
            return jsonify({"error": "Invalid JSON response from GPT"}), 500

        # BigQueryã«ä¿å­˜
        table_id = "llm_advicebot.llm_advice_makino"
        
        # advice_dataãŒãƒªã‚¹ãƒˆã®å ´åˆã¯å„è¦ç´ ã‚’å‡¦ç†ã€è¾žæ›¸ã®å ´åˆã¯ãƒªã‚¹ãƒˆã«å¤‰æ›
        if isinstance(advice_data, list):
            advice_list = advice_data
        elif isinstance(advice_data, dict):
            advice_list = [advice_data]
        else:
            print(f"âŒ Unexpected data structure: {type(advice_data)}")
            return jsonify({"error": "Unexpected response structure from GPT"}), 500

        # å„IDã®ãƒ‡ãƒ¼ã‚¿ã‚’BigQueryã«ä¿å­˜
        rows_to_insert = []
        for advice_item in advice_list:
            if isinstance(advice_item, dict) and "id" in advice_item:
                row = {
                    "summary_date": date.today().isoformat(),
                    "participant_uid": advice_item.get("id", ""),
                    "sleep_analysis": advice_item.get("sleep_analysis", ""),
                    "activity_analysis": advice_item.get("activity_analysis", ""),
                    "recommendations": advice_item.get("recommendations", ""),
                    "overall_assessment": advice_item.get("overall_assessment", "")
                }
                rows_to_insert.append(row)
                print(f"âœ… Prepared data for ID: {advice_item.get('id', '')}")
            else:
                print(f"âŒ Skipping invalid advice item: {advice_item}")

        if rows_to_insert:
            print(f"ðŸ”„ Inserting {len(rows_to_insert)} records to BigQuery")
            errors = bigquery_client.insert_rows_json(table_id, rows_to_insert)
            if errors:
                print(f"âŒ Failed to insert rows: {errors}")
                return jsonify({"error": "BigQuery insert failed", "details": errors}), 500
            print(f"âœ… {len(rows_to_insert)} records saved to BigQuery")
        else:
            print("âŒ No valid data to insert")

        # GPTã®å¿œç­”ã‚’è¿”å´
        return jsonify(advice_data)
    except Exception as e:
        print(f"âŒ Exception occurred: {e}")
        return jsonify({"error": str(e)}), 500